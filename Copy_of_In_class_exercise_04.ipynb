{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of In-class-exercise-04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vyshnav23/vyshnav_INFO5731_Spring2020/blob/master/Copy_of_In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw",
        "colab_type": "text"
      },
      "source": [
        "# **The fourth in-class-exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f",
        "colab_type": "text"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above.\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 2-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR0L3_CreM_A",
        "colab_type": "code",
        "outputId": "14406467-f7c2-4cc5-bc76-82afe2b6d65e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Write your code here\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop=stopwords.words('english')\n",
        "file = open(\"data.txt\",\"r\",encoding=\"utf8\")\n",
        "sentences=0\n",
        "wordcount=0\n",
        "characters=0\n",
        "word_length=0\n",
        "stop_words=0\n",
        "special=0\n",
        "numerics=0\n",
        "uppercase=0\n",
        "for line in file:\n",
        "     sentences+=line.count('.')+line.count('!')+line.count('?')\n",
        "     word= line.split()\n",
        "     for w in word:\n",
        "        \n",
        "         word_length+=len(w)\n",
        "         if w.isupper():\n",
        "            uppercase=uppercase+1;\n",
        "         if w in stop: \n",
        "            stop_words=stop_words+1\n",
        "         for i in range(len(w)):\n",
        "            if(w[i].isalpha()):\n",
        "                continue\n",
        "            elif(w[i].isdigit()):\n",
        "                numerics=numerics+1\n",
        "            else:\n",
        "                special = special + 1\n",
        "     wordcount+=len(word)\n",
        "     characters+=len(line)\n",
        "print('number of sentences',sentences)\n",
        "print('Number of words in text file :', wordcount)\n",
        "print('Number of characters in text file :', characters)\n",
        "print('Number of average word length in text file :', word_length/wordcount)\n",
        "print('Number of stop words in text file :', stop_words)\n",
        "print('Number of special characters in text file :', special)\n",
        "print('Number of numerical characters in text file :', numerics)\n",
        "print('Number of uppercase words in text file :', uppercase)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "number of sentences 292\n",
            "Number of words in text file : 3707\n",
            "Number of characters in text file : 20453\n",
            "Number of average word length in text file : 4.510385756676558\n",
            "Number of stop words in text file : 1679\n",
            "Number of special characters in text file : 817\n",
            "Number of numerical characters in text file : 356\n",
            "Number of uppercase words in text file : 84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw1SCd3X5YAh",
        "colab_type": "code",
        "outputId": "5eda7c34-0de0-44a8-e8a2-4c3c1ad2ef69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop=stopwords.words('english')\n",
        "file = open(\"data.txt\",\"r+\",encoding=\"utf8\")\n",
        "with open('csvfile.csv','wb') as csvfile:\n",
        " count=0\n",
        " punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        " for line in file:\n",
        "     sentences+=line.count('.')+line.count('!')+line.count('?')\n",
        "     word= line.split()\n",
        "     for w in word:\n",
        "       w=\"\".join(w.lower())\n",
        "     csvfile.write(word)\n",
        "     csvfile.write('\\n')\n",
        "     for i_word in word:\n",
        "        for x in i_word: \n",
        "         if x in punctuations: \n",
        "            i_word = i_word.replace(x, \"\")\n",
        "     csvfile.write(word)\n",
        "     csvfile.write('\\n')\n",
        "     for in_word in word:\n",
        "          if in_word in stop: \n",
        "            in_word=\"\"\n",
        "     csvfile.write(word)\n",
        "     csvfile.write('\\n')\n",
        "     for fre in word:\n",
        "         count=0\n",
        "         i=0\n",
        "         for w1 in word:\n",
        "            if w1 is fre and (count==10):\n",
        "               i=i+1\n",
        "               w1=\"\"\n",
        "            else:\n",
        "               count=count+1;\n",
        "         csvfile.write(word)\n",
        "         csvfile.write('\\n')\n",
        "          \n",
        "\n",
        "\n",
        "     \n",
        "\n",
        "      \n",
        "\n",
        "        \n",
        "         "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-9d8ec647c23b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m        \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m      \u001b[0mcsvfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m      \u001b[0mcsvfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mi_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV",
        "colab_type": "text"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. \n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSv6fVhOfFmv",
        "colab_type": "code",
        "outputId": "c7d54047-fe53-4913-da4f-a93d6162a867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Write your code here\n",
        "import re\n",
        "ip = \"260.08.094.109\"\n",
        "string = re.sub('\\.[0]+', '.', ip)\n",
        "print(string)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence.\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xdJpDx9gjbX",
        "colab_type": "code",
        "outputId": "205c4d43-5f78-4d7e-f671-79e6654526e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Write your code here\n",
        "import re \n",
        "input_string = \"\"\"The 2010s were a dramatic decade, filled with ups and downs, \n",
        "more than 1000 stroies have happened. As the decade comes to a close, \n",
        "Insider took a look back at some of the biggest headline-grabbing stories, \n",
        "from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys'\n",
        " soccer team from a flooded cave to the divisive election of President Donald Trump.\"\"\"\n",
        "\n",
        "temp = re.findall(r'2\\d{3}', input_string) \n",
        "print(temp)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2010', '2010', '2019']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}