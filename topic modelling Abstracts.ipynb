{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTl6zhZTvv0tgnZzHtGG4m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vyshnav23/vyshnav_INFO5731_Spring2020/blob/master/topic%20modelling%20Abstracts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5SajXGNxTMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df3 = pd.read_csv('Final_Table_3.csv',low_memory=False)\n",
        "df3_title = df3['title']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGIWRn_axjHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for removing stop words and stemming\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "nltk.download('stopwords')\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# create English stop words list\n",
        "en_stop = stopwords.words('english')\n",
        "\n",
        "# Create p_stemmer of class PorterStemmer\n",
        "p_stemmer = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHw0kWDUxoz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# appending the abstracts and titles of every five years into a list\n",
        "title_doc = []\n",
        "abstract_doc = []\n",
        "for i in range(1971,2017,5):\n",
        "  print()\n",
        "  title_df = df3.loc[(df3['year'] >=i) & (df3['year']<i+5),'title']\n",
        "  #abstract_df = df3.loc[(df3['Year'] >=i) & (df3['Year']<i+5),'Abstract']\n",
        "\n",
        "  title_list = title_df.tolist()\n",
        "  title = ''.join(title_list)\n",
        "  title_doc.append(title)\n",
        "\n",
        "   abstract_list = abstract_df.tolist()\n",
        "   abstract = ''.join(abstract_list)\n",
        "   abstract_doc.append(abstract)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FntCgLlxuFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting Keywords from titles\n",
        "for i in range(len(title_doc)):\n",
        "    \n",
        "    \n",
        "    # clean and tokenize document string\n",
        "    raw_title = title_doc[i].lower()\n",
        "    title_tokens = tokenizer.tokenize(raw_title)\n",
        "\n",
        "\n",
        "    # remove stop words from tokens\n",
        "    stopped_title_tokens = [i for i in title_tokens if not i in en_stop]\n",
        "    \n",
        "    # stem tokens\n",
        "    # stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "    title_texts = []\n",
        "    # add tokens to list\n",
        "    title_texts.append(stopped_title_tokens)\n",
        "\n",
        "    # turn our tokenized documents into a id <-> term dictionary\n",
        "    title_dictionary = corpora.Dictionary(title_texts)\n",
        "\n",
        "        \n",
        "    # convert tokenized documents into a document-term matrix\n",
        "    title_corpus = [title_dictionary.doc2bow(title) for title in title_texts]\n",
        "\n",
        "    # generate LDA model\n",
        "    title_ldamodel = gensim.models.ldamodel.LdaModel(title_corpus, num_topics=1,id2word = title_dictionary, passes=20)\n",
        "    \n",
        "    # lda_pred.append(title_ldamodel.print_topics())\n",
        "    print(title_ldamodel.print_topics(num_words = 5))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1TEVXRQxxN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting Keywords from Abstracts\n",
        "for i in range(len(abstract_doc)):\n",
        "    \n",
        "    \n",
        "    # clean and tokenize document string\n",
        "    raw_abstract = abstract_doc[i].lower()\n",
        "    abstract_tokens = tokenizer.tokenize(raw_abstract)\n",
        "\n",
        "\n",
        "    # remove stop words from tokens\n",
        "    stopped_abstract_tokens = [i for i in abstract_tokens if not i in en_stop]\n",
        "    \n",
        "    # stem tokens\n",
        "    # stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "    abstract_texts = []\n",
        "    # add tokens to list\n",
        "    abstract_texts.append(stopped_abstract_tokens)\n",
        "\n",
        "    # turn our tokenized documents into a id <-> term dictionary\n",
        "    abstract_dictionary = corpora.Dictionary(abstract_texts)\n",
        "\n",
        "        \n",
        "    # convert tokenized documents into a document-term matrix\n",
        "    abstract_corpus = [abstract_dictionary.doc2bow(abstract) for abstract in abstract_texts]\n",
        "\n",
        "    # generate LDA model\n",
        "    abstract_ldamodel = gensim.models.ldamodel.LdaModel(abstract_corpus, num_topics=1,id2word = abstract_dictionary, passes=20)\n",
        "    \n",
        "    # lda_pred.append(title_ldamodel.print_topics())\n",
        "    print(abstract_ldamodel.print_topics(num_words = 5))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}